# Load the 'prophet' library
library(prophet)
data <- read_csv("hhs_cdc_1_18_24.csv")

# Assuming 'pediatric_hosp_flu_est' is the target variable
target_variable <- 'pediatric_hosp_flu_est'

# Create a data frame with 'ds' (date) and 'y' (target variable)
prophet_data <- data.frame(ds = data$date1, y = data[[target_variable]])

# Split dataset into all even weeks for training and all odd weeks for testing
train_prophet <- subset(prophet_data, data$`MMWR.WEEK` %% 2 == 0)
test_prophet <- subset(prophet_data, data$`MMWR.WEEK` %% 2 != 0)

# Train Prophet model with weekly and yearly seasonality
prophet_model <- prophet(yearly.seasonality = TRUE,weekly.seasonality = FALSE, holidays.prior.scale = 10)
prophet_model <- add_seasonality(prophet_model, name='monthly', period=30.5, fourier.order=5)
# Fit the model
prophet_model <- fit.prophet(prophet_model, train_prophet)

# Create a data frame with future dates for prediction
future_prophet <- make_future_dataframe(prophet_model, periods = nrow(test_prophet), freq = 'weeks')

# Predict on the test set
forecast_prophet <- predict(prophet_model, future_prophet)
predictions_prophet <- forecast_prophet$yhat[nrow(train_prophet) + 1:nrow(test_prophet)]

predictions_prophet <- predictions_prophet[!is.na(predictions_prophet)]

plot(prophet_model, forecast_prophet, xlabel = 'Date', ylabel = target_variable)

cv_results <- cross_validation(prophet_model, initial = 730, horizon = 21, units = "days")

# Print cross-validation results
print(cv_results)

# Calculate performance metrics
performancemetrics <- performance_metrics(cv_results)
print(performancemetrics)


##################adding additional features 
merged_data <- read_csv("merged_data.csv")
merged_data$flu <- as.numeric(merged_data$flu)
glimpse(merged_data)
# Adjust Prophet model to include additional regressor
prophet_model <- prophet(yearly.seasonality = TRUE, weekly.seasonality = FALSE,  holidays.prior.scale = 10)
prophet_model <- add_seasonality(prophet_model, name='monthly', period=30.5, fourier.order=5)

# Add the additional regressor
prophet_model <- add_regressor(prophet_model,'flu')

# Create a data frame for training
prophet_data <- data.frame(ds = merged_data$date, 
                           y = merged_data$pediatric_hosp_flu_est, 
                          flu = merged_data$flu)

# Split dataset into training and testing
train_prophet <- subset(prophet_data, merged_data$`MMWR.WEEK` %% 2 == 0)
test_prophet <- subset(prophet_data, merged_data$`MMWR.WEEK` %% 2 != 0)

# Fit the model
prophet_model <- fit.prophet(prophet_model, train_prophet)

cv_results <- cross_validation(prophet_model, initial = 730, horizon = 21, units = "days")

# Print cross-validation results
print(cv_results)

# Calculate performance metrics
performancemetrics <- performance_metrics(cv_results)
print(performancemetrics)


# Create a dataframe with future dates for prediction
future_prophet <- make_future_dataframe(prophet_model, periods = nrow(test_prophet), freq = 'weeks')

future_prophet$flu <- test_prophet$flu[1:nrow(future_prophet)]
# Predict on the test set
forecast_prophet <- predict(prophet_model, future_prophet)

# Plot the forecast
plot(prophet_model, forecast_prophet, xlabel = 'Date', ylabel = 'Pediatric Flu Hospitalizations')

# Fit the model
prophet_model <- fit.prophet(prophet_model, train_prophet)

# Create a data frame with future dates for prediction
future_prophet <- make_future_dataframe(prophet_model, periods = nrow(test_prophet), freq = 'weeks')

# Predict on the test set
forecast_prophet <- predict(prophet_model, future_prophet)
predictions_prophet <- forecast_prophet$yhat[nrow(train_prophet) + 1:nrow(test_prophet)]

predictions_prophet <- predictions_prophet[!is.na(predictions_prophet)]

plot(prophet_model, forecast_prophet, xlabel = 'Date', ylabel = target_variable)

